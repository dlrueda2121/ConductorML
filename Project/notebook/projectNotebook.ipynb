{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECS 171 Group Project - Group 10 - ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean/Wrangle Data\n",
    "\n",
    "\"Process the data the same way the paper author did\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "(21263, 82)\n",
      "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
      "0                   4         88.944468             57.862692   \n",
      "1                   5         92.729214             58.518416   \n",
      "2                   4         88.944468             57.885242   \n",
      "3                   4         88.944468             57.873967   \n",
      "4                   4         88.944468             57.840143   \n",
      "\n",
      "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
      "0          66.361592              36.116612             1.181795   \n",
      "1          73.132787              36.396602             1.449309   \n",
      "2          66.361592              36.122509             1.181795   \n",
      "3          66.361592              36.119560             1.181795   \n",
      "4          66.361592              36.110716             1.181795   \n",
      "\n",
      "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
      "0                 1.062396          122.90607              31.794921   \n",
      "1                 1.057755          122.90607              36.161939   \n",
      "2                 0.975980          122.90607              35.741099   \n",
      "3                 1.022291          122.90607              33.768010   \n",
      "4                 1.129224          122.90607              27.848743   \n",
      "\n",
      "   std_atomic_mass  wtd_std_atomic_mass  mean_fie  wtd_mean_fie   gmean_fie  \\\n",
      "0        51.968828            53.622535   775.425   1010.268571  718.152900   \n",
      "1        47.094633            53.979870   766.440   1010.612857  720.605511   \n",
      "2        51.968828            53.656268   775.425   1010.820000  718.152900   \n",
      "3        51.968828            53.639405   775.425   1010.544286  718.152900   \n",
      "4        51.968828            53.588771   775.425   1009.717143  718.152900   \n",
      "\n",
      "   wtd_gmean_fie  entropy_fie  wtd_entropy_fie  range_fie  wtd_range_fie  \\\n",
      "0     938.016780     1.305967         0.791488      810.6     735.985714   \n",
      "1     938.745413     1.544145         0.807078      810.6     743.164286   \n",
      "2     939.009036     1.305967         0.773620      810.6     743.164286   \n",
      "3     938.512777     1.305967         0.783207      810.6     739.575000   \n",
      "4     937.025573     1.305967         0.805230      810.6     728.807143   \n",
      "\n",
      "      std_fie  wtd_std_fie  mean_atomic_radius  wtd_mean_atomic_radius  \\\n",
      "0  323.811808   355.562967              160.25              105.514286   \n",
      "1  290.183029   354.963511              161.20              104.971429   \n",
      "2  323.811808   354.804183              160.25              104.685714   \n",
      "3  323.811808   355.183884              160.25              105.100000   \n",
      "4  323.811808   356.319281              160.25              106.342857   \n",
      "\n",
      "   gmean_atomic_radius  wtd_gmean_atomic_radius  entropy_atomic_radius  \\\n",
      "0           136.126003                84.528423               1.259244   \n",
      "1           141.465215                84.370167               1.508328   \n",
      "2           136.126003                84.214573               1.259244   \n",
      "3           136.126003                84.371352               1.259244   \n",
      "4           136.126003                84.843442               1.259244   \n",
      "\n",
      "   wtd_entropy_atomic_radius  range_atomic_radius  wtd_range_atomic_radius  \\\n",
      "0                   1.207040                  205                42.914286   \n",
      "1                   1.204115                  205                50.571429   \n",
      "2                   1.132547                  205                49.314286   \n",
      "3                   1.173033                  205                46.114286   \n",
      "4                   1.261194                  205                36.514286   \n",
      "\n",
      "   std_atomic_radius  wtd_std_atomic_radius  mean_Density  wtd_mean_Density  \\\n",
      "0          75.237540              69.235569    4654.35725       2961.502286   \n",
      "1          67.321319              68.008817    5821.48580       3021.016571   \n",
      "2          75.237540              67.797712    4654.35725       2999.159429   \n",
      "3          75.237540              68.521665    4654.35725       2980.330857   \n",
      "4          75.237540              70.634448    4654.35725       2923.845143   \n",
      "\n",
      "   gmean_Density  wtd_gmean_Density  entropy_Density  wtd_entropy_Density  \\\n",
      "0     724.953211          53.543811         1.033129             0.814598   \n",
      "1    1237.095080          54.095718         1.314442             0.914802   \n",
      "2     724.953211          53.974022         1.033129             0.760305   \n",
      "3     724.953211          53.758486         1.033129             0.788889   \n",
      "4     724.953211          53.117029         1.033129             0.859811   \n",
      "\n",
      "   range_Density  wtd_range_Density  std_Density  wtd_std_Density  \\\n",
      "0       8958.571        1579.583429  3306.162897      3572.596624   \n",
      "1      10488.571        1667.383429  3767.403176      3632.649185   \n",
      "2       8958.571        1667.383429  3306.162897      3592.019281   \n",
      "3       8958.571        1623.483429  3306.162897      3582.370597   \n",
      "4       8958.571        1491.783429  3306.162897      3552.668664   \n",
      "\n",
      "   mean_ElectronAffinity  wtd_mean_ElectronAffinity  gmean_ElectronAffinity  \\\n",
      "0                81.8375                 111.727143               60.123179   \n",
      "1                90.8900                 112.316429               69.833315   \n",
      "2                81.8375                 112.213571               60.123179   \n",
      "3                81.8375                 111.970357               60.123179   \n",
      "4                81.8375                 111.240714               60.123179   \n",
      "\n",
      "   wtd_gmean_ElectronAffinity  entropy_ElectronAffinity  \\\n",
      "0                   99.414682                  1.159687   \n",
      "1                  101.166398                  1.427997   \n",
      "2                  101.082152                  1.159687   \n",
      "3                  100.244950                  1.159687   \n",
      "4                   97.774719                  1.159687   \n",
      "\n",
      "   wtd_entropy_ElectronAffinity  range_ElectronAffinity  \\\n",
      "0                      0.787382                  127.05   \n",
      "1                      0.838666                  127.05   \n",
      "2                      0.786007                  127.05   \n",
      "3                      0.786900                  127.05   \n",
      "4                      0.787396                  127.05   \n",
      "\n",
      "   wtd_range_ElectronAffinity  std_ElectronAffinity  wtd_std_ElectronAffinity  \\\n",
      "0                   80.987143             51.433712                 42.558396   \n",
      "1                   81.207857             49.438167                 41.667621   \n",
      "2                   81.207857             51.433712                 41.639878   \n",
      "3                   81.097500             51.433712                 42.102344   \n",
      "4                   80.766429             51.433712                 43.452059   \n",
      "\n",
      "   mean_FusionHeat  wtd_mean_FusionHeat  gmean_FusionHeat  \\\n",
      "0           6.9055             3.846857          3.479475   \n",
      "1           7.7844             3.796857          4.403790   \n",
      "2           6.9055             3.822571          3.479475   \n",
      "3           6.9055             3.834714          3.479475   \n",
      "4           6.9055             3.871143          3.479475   \n",
      "\n",
      "   wtd_gmean_FusionHeat  entropy_FusionHeat  wtd_entropy_FusionHeat  \\\n",
      "0              1.040986            1.088575                0.994998   \n",
      "1              1.035251            1.374977                1.073094   \n",
      "2              1.037439            1.088575                0.927479   \n",
      "3              1.039211            1.088575                0.964031   \n",
      "4              1.044545            1.088575                1.044970   \n",
      "\n",
      "   range_FusionHeat  wtd_range_FusionHeat  std_FusionHeat  wtd_std_FusionHeat  \\\n",
      "0            12.878              1.744571        4.599064            4.666920   \n",
      "1            12.878              1.595714        4.473363            4.603000   \n",
      "2            12.878              1.757143        4.599064            4.649635   \n",
      "3            12.878              1.744571        4.599064            4.658301   \n",
      "4            12.878              1.744571        4.599064            4.684014   \n",
      "\n",
      "   mean_ThermalConductivity  wtd_mean_ThermalConductivity  \\\n",
      "0                107.756645                     61.015189   \n",
      "1                172.205316                     61.372331   \n",
      "2                107.756645                     60.943760   \n",
      "3                107.756645                     60.979474   \n",
      "4                107.756645                     61.086617   \n",
      "\n",
      "   gmean_ThermalConductivity  wtd_gmean_ThermalConductivity  \\\n",
      "0                   7.062488                       0.621979   \n",
      "1                  16.064228                       0.619735   \n",
      "2                   7.062488                       0.619095   \n",
      "3                   7.062488                       0.620535   \n",
      "4                   7.062488                       0.624878   \n",
      "\n",
      "   entropy_ThermalConductivity  wtd_entropy_ThermalConductivity  \\\n",
      "0                     0.308148                         0.262848   \n",
      "1                     0.847404                         0.567706   \n",
      "2                     0.308148                         0.250477   \n",
      "3                     0.308148                         0.257045   \n",
      "4                     0.308148                         0.272820   \n",
      "\n",
      "   range_ThermalConductivity  wtd_range_ThermalConductivity  \\\n",
      "0                  399.97342                      57.127669   \n",
      "1                  429.97342                      51.413383   \n",
      "2                  399.97342                      57.127669   \n",
      "3                  399.97342                      57.127669   \n",
      "4                  399.97342                      57.127669   \n",
      "\n",
      "   std_ThermalConductivity  wtd_std_ThermalConductivity  mean_Valence  \\\n",
      "0               168.854244                   138.517163          2.25   \n",
      "1               198.554600                   139.630922          2.00   \n",
      "2               168.854244                   138.540613          2.25   \n",
      "3               168.854244                   138.528893          2.25   \n",
      "4               168.854244                   138.493671          2.25   \n",
      "\n",
      "   wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  entropy_Valence  \\\n",
      "0          2.257143       2.213364           2.219783         1.368922   \n",
      "1          2.257143       1.888175           2.210679         1.557113   \n",
      "2          2.271429       2.213364           2.232679         1.368922   \n",
      "3          2.264286       2.213364           2.226222         1.368922   \n",
      "4          2.242857       2.213364           2.206963         1.368922   \n",
      "\n",
      "   wtd_entropy_Valence  range_Valence  wtd_range_Valence  std_Valence  \\\n",
      "0             1.066221              1           1.085714     0.433013   \n",
      "1             1.047221              2           1.128571     0.632456   \n",
      "2             1.029175              1           1.114286     0.433013   \n",
      "3             1.048834              1           1.100000     0.433013   \n",
      "4             1.096052              1           1.057143     0.433013   \n",
      "\n",
      "   wtd_std_Valence  critical_temp  \n",
      "0         0.437059           29.0  \n",
      "1         0.468606           26.0  \n",
      "2         0.444697           19.0  \n",
      "3         0.440952           22.0  \n",
      "4         0.428809           23.0  \n",
      "\n",
      "Chemical formulas\n",
      "(21263, 88)\n",
      "     H  He   Li   Be    B    C    N    O    F  Ne   Na   Mg   Al   Si    P  \\\n",
      "0  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  0.0  0.0  0.0  0.0  0.0   \n",
      "1  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  0.0  0.0  0.0  0.0  0.0   \n",
      "2  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  0.0  0.0  0.0  0.0  0.0   \n",
      "3  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  0.0  0.0  0.0  0.0  0.0   \n",
      "4  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "     S   Cl  Ar    K   Ca   Sc   Ti    V   Cr   Mn   Fe   Co   Ni   Cu   Zn  \\\n",
      "0  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "1  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.9  0.0   \n",
      "2  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "3  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "4  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  1.0  0.0   \n",
      "\n",
      "    Ga   Ge   As   Se   Br  Kr   Rb   Sr    Y   Zr   Nb   Mo   Tc   Ru   Rh  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "    Pd   Ag   Cd   In   Sn   Sb   Te    I  Xe   Cs    Ba    La   Ce   Pr   Nd  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0  0.0  0.20  1.80  0.0  0.0  0.0   \n",
      "1  0.0  0.1  0.0  0.0  0.0  0.0  0.0  0.0   0  0.0  0.10  1.90  0.0  0.0  0.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0  0.0  0.10  1.90  0.0  0.0  0.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0  0.0  0.15  1.85  0.0  0.0  0.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0  0.0  0.30  1.70  0.0  0.0  0.0   \n",
      "\n",
      "   Pm   Sm   Eu   Gd   Tb   Dy   Ho   Er   Tm   Yb   Lu   Hf   Ta    W   Re  \\\n",
      "0   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "1   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "2   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "3   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "4   0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
      "\n",
      "    Os   Ir   Pt   Au   Hg   Tl   Pb   Bi  Po  At  Rn  critical_temp  \\\n",
      "0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0           29.0   \n",
      "1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0           26.0   \n",
      "2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0           19.0   \n",
      "3  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0           22.0   \n",
      "4  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   0   0   0           23.0   \n",
      "\n",
      "                 material  \n",
      "0         Ba0.2La1.8Cu1O4  \n",
      "1  Ba0.1La1.9Ag0.1Cu0.9O4  \n",
      "2         Ba0.1La1.9Cu1O4  \n",
      "3       Ba0.15La1.85Cu1O4  \n",
      "4         Ba0.3La1.7Cu1O4  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Dense\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "#Contains 81 features extracted from 21263 superconductors along with the critical temperature in the 82nd column\n",
    "train = pd.read_csv('./../superconduct/train.csv')\n",
    "\n",
    "#Contains the chemical formula broken up for all the 21263 superconductors from the train.csv file.\n",
    "#The last two columns have the critical temperature and chemical formula.\n",
    "formula = pd.read_csv('./../superconduct/unique_m.csv')\n",
    "\n",
    "print(\"Training Data\")\n",
    "print(train.shape)\n",
    "print(train.head())\n",
    "\n",
    "print(\"\\nChemical formulas\")\n",
    "print(formula.shape)\n",
    "print(formula.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[730.9123, 306.80493, 283.52402, 277.9772, 265.90268, 256.90247, 255.68286, 248.70401, 253.75, 246.84033, 236.82263, 240.22174, 234.98877, 231.10246, 232.4681, 232.47356, 230.60283, 222.74098, 217.0233, 224.076, 223.16472, 218.09166, 219.9837, 221.38756, 207.82469, 213.5334, 210.43912, 198.88345, 202.33267, 204.06165, 202.67572, 201.59964, 196.08148, 199.43335, 202.9393, 191.47571, 193.50215, 195.12576, 191.27057, 192.27007, 187.22926, 197.5045, 188.39064, 189.1852, 187.21954, 184.55386, 180.24951, 186.45096, 181.1169, 187.26868, 183.01054, 182.10034, 181.29337, 177.6907, 175.00479, 176.69911, 171.35371, 177.86, 170.2244, 169.14676, 172.64749, 173.61957, 168.25328, 172.80429, 168.51996, 168.20311, 173.36743, 167.75752, 165.52852, 163.90337, 164.52597, 161.82188, 169.00737, 160.85062, 163.3417, 162.4364, 162.13432, 153.56905, 160.08365, 157.36469, 151.93549, 157.34872, 166.22438, 153.89787, 152.58347, 157.02328, 160.69502, 151.38185, 156.29698, 151.54619, 152.2801, 150.1811, 155.67082, 149.92418, 153.0797, 148.28537, 152.30489, 155.2106, 145.42621, 144.57935, 150.31212, 147.36807, 148.86273, 144.61555, 143.55164, 151.68622, 143.2089, 142.54478, 150.08516, 144.2623, 139.14445, 138.40044, 140.68925, 142.06956, 144.57782, 139.92715, 142.65671, 141.4042, 140.34402, 138.95396, 140.37076, 140.37332, 143.45598, 136.47217, 140.24529, 139.02197, 133.06956, 137.04088, 141.36467, 138.86166, 139.93077, 134.89447, 140.04126, 132.90945, 129.07193, 131.90248, 134.88031, 128.20468, 136.52206, 128.30507, 134.82152, 130.30461, 132.87546, 127.28995, 130.17401, 126.657364, 130.66304, 130.38397, 124.344864, 131.4786, 127.08831, 123.36145, 125.61836, 126.17135, 127.351105, 129.1448, 125.0814, 128.89845, 123.28344, 129.2045, 123.92411, 128.06078, 120.55018, 123.52283, 126.74625, 130.35524, 124.37508, 122.08884, 123.63803, 120.51345, 119.74245, 119.41032, 123.76032, 117.33327, 117.83993, 116.76491, 122.219574, 115.31537, 117.84637, 119.97385, 116.383514, 120.42297, 116.57927, 115.57553, 119.23361, 112.92768, 115.82426, 110.01215, 119.27667, 121.287384, 116.748505, 112.77572, 117.2559, 115.9826, 114.67925, 117.822395, 114.85815, 113.91063, 110.86969, 113.482895, 111.383316, 110.539185, 115.37586, 107.34566, 107.83623, 107.892426, 105.74412, 111.64419, 107.0411, 111.43678, 109.68589, 104.95293, 109.93234, 108.9232, 107.03605, 106.32915, 105.770744, 105.7399, 105.89332, 112.461685, 107.2797, 103.99245, 104.77023, 104.65505, 107.83406, 107.55992, 104.27911, 108.35416, 100.265076, 104.1355, 100.79181, 103.2713, 102.987366, 106.10147, 102.45861, 98.81556, 100.37095, 103.377975, 96.750336, 105.47511, 99.0129, 110.187614, 104.91281, 95.26622, 102.31273, 97.6612, 100.54916, 100.57259, 98.936615, 100.99586, 101.859474, 99.09113, 108.57936, 102.682846, 99.09575, 96.62402, 95.04234, 96.63885, 97.59098, 95.7935, 98.22178, 97.27161, 95.907745, 98.194954, 92.279396, 100.491585, 98.55878, 92.275986, 95.6696, 101.27519, 93.407265, 98.398285, 96.13473, 98.07935, 94.742386, 100.392784, 94.32817, 96.77367, 91.47714, 98.14675, 93.17008, 97.12975, 95.562935, 94.6243, 91.920815, 91.70577, 88.56422, 88.72653, 88.58987, 99.3161, 90.07953, 90.33907, 92.06366, 89.89706, 95.090836, 95.304146, 90.92492, 87.01692, 91.95284, 96.72973, 87.90964, 89.88363, 89.10252, 89.44659, 89.53002, 91.4897, 91.885414, 89.15203, 86.93748, 88.38611, 87.274055, 92.71749, 91.41447, 91.861465, 90.42227, 90.42008, 90.05705, 89.66205, 92.758835, 87.76468, 90.831436, 91.591995, 97.397964, 90.68701, 88.01013, 85.87668, 81.436424, 94.98069, 84.6639, 88.71247, 88.81024, 87.42542, 87.03168, 88.547264, 87.726494, 94.47349, 97.91919, 84.47783, 83.70235, 84.477425, 86.31292, 95.72259, 89.43217, 82.33283, 85.54835, 91.96305, 83.32199, 88.432915, 82.82281, 83.91017, 85.55113, 85.0245, 84.61348, 86.675934, 84.55065, 81.58752, 83.03486, 82.52281, 83.74366, 84.580315, 90.58455, 87.53049, 82.57922, 81.40419, 84.740715, 83.670555, 80.80725, 84.02124, 85.90535, 84.06277, 80.3765, 85.18586, 83.05491, 83.834076, 85.453255, 86.758385, 81.24925, 84.30477, 82.22305, 89.20327, 82.88423, 82.52053, 88.01968, 83.907364, 85.100296, 80.920555, 80.727325, 80.520195, 81.017, 78.6003, 81.3915, 81.52698, 81.44226, 80.58497, 81.90667, 86.44107, 80.54672, 80.30959, 87.26351, 83.641335, 78.79085, 78.131195, 78.57757, 92.06997, 83.71913, 88.47308, 83.435585, 80.29993, 77.12853, 78.58785, 78.06237, 85.03657, 79.0728, 78.712746, 80.779915, 77.99166, 77.177635, 79.883865, 77.32505, 78.74414, 82.178085, 74.37123, 76.25252, 74.79584, 82.32719, 78.56612, 80.953156, 78.024086, 79.45442, 79.67241, 74.939926, 77.48597, 80.641235, 78.58997, 76.43602, 78.55824, 77.55305, 75.73582, 78.28344, 82.36011, 75.770065, 82.00005, 78.51267, 77.73819, 76.53361, 77.36964, 73.26595, 82.59753, 80.0815, 72.901115, 73.02244, 82.27293, 76.16212, 76.24509, 74.66, 73.28241, 80.5657, 74.32078, 73.69533, 78.443085, 79.62823, 78.655304, 72.796684, 75.80618, 78.7262, 75.18114, 72.420845, 72.955505, 80.85312, 72.834145, 72.58214, 72.33458, 72.54927, 77.140236, 73.70542, 78.38509, 76.5729, 73.23956, 71.39558, 73.67566, 72.482254, 73.80422, 72.98745, 76.28678, 73.10883, 73.16938, 72.25558, 79.57491, 71.69099, 73.835945, 71.60094, 80.00727, 72.61722, 78.672165, 71.43901, 70.43077, 71.449196, 72.422325, 70.74199, 76.67661, 81.95344, 73.83722, 70.62081, 77.53816, 70.91765, 75.92067, 76.9165, 74.42617, 71.58558, 71.09759, 78.421394, 69.39586, 72.33742, 70.91491, 72.95101, 71.62531, 75.78252, 77.76814, 77.24189, 72.58845, 76.85112, 71.060005, 72.17931, 72.98833, 71.39842, 70.915016, 80.86273, 67.542755, 73.72013, 69.22493, 76.05575, 71.507614, 73.74361, 72.417465, 70.5831, 71.211975, 70.20949, 69.58738, 75.7523, 69.63269, 72.064224, 71.90327, 74.18916, 72.09534, 70.39612, 72.46277, 69.828964, 70.74541, 70.85981, 69.984406, 73.89744, 68.82688, 70.779236, 66.72463, 74.42141, 70.96157, 70.30269, 68.36177, 70.40161, 73.96626, 67.551605, 79.69898, 72.11264, 69.435, 65.99383, 69.6189, 71.80318, 73.00191, 70.01171, 71.84775, 69.28814, 68.81681, 69.875404, 69.80211, 69.71465, 66.36105, 74.053154, 70.579834, 71.57449, 72.68281, 72.76083, 67.03013, 69.19902, 67.53065, 70.12288, 72.86339, 67.75819, 76.75293, 67.78511, 65.28104, 67.02428, 75.22791, 70.613235, 72.64457, 71.97124, 72.75408, 65.768234, 70.50427, 69.78882, 69.367905, 67.576515, 68.61474, 69.61717, 69.436745, 71.281715, 69.78041, 71.30528, 68.35571, 63.675137, 69.5372, 70.05381, 67.77531, 64.12901, 70.67815, 68.9786, 66.48951, 64.7341, 67.7044, 67.01553, 66.31068, 68.171616, 65.29168, 67.15383, 70.85374, 66.64653, 69.90519, 67.30457, 64.60524, 69.21048, 67.8001, 69.478745, 67.20892, 64.615944, 64.80732, 71.23887, 66.62789, 65.338234, 67.665405, 68.4143, 66.56846, 67.540436, 74.92151, 65.13723, 66.8766, 64.3297, 67.99058, 67.49624, 67.11586, 68.09915, 66.259796, 68.88297, 67.83107, 64.740685, 67.684074, 65.2818, 69.450294, 67.98093, 66.60415, 68.31587, 65.59142, 68.453804, 67.943405, 69.667046, 64.9073, 64.653336, 63.624557, 67.000244, 65.88933, 73.61728, 64.29336, 64.87639, 66.40234, 65.71775, 68.79182, 65.13199, 65.79773, 63.75732, 63.340527, 64.39427, 65.73171, 63.8138, 63.56904, 68.41091, 63.157764, 66.57574, 67.724174, 67.312965, 65.89703, 62.496643, 64.40715, 67.71956, 64.04977, 67.54877, 62.88546, 63.80114, 65.2768, 66.259346, 63.48129, 65.518974, 62.70932]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "trainx = train.drop(\"critical_temp\",axis=1)\n",
    "trainy = train['critical_temp']\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(trainx,trainy, train_size=0.7, random_state=1)\n",
    "\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "train_X = sc.fit_transform(train_X)\n",
    "test_X = sc.fit_transform(test_X)\n",
    "\n",
    "nn = Sequential(\n",
    "    [\n",
    "        keras.Input(shape=train_X.shape[1]),\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(100, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(60, activation='relu'),\n",
    "        layers.Dense(20, activation='relu'),\n",
    "        layers.Dense(1)\n",
    "    ]\n",
    ")\n",
    "\n",
    "nn.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), \n",
    "           loss=\"mean_squared_error\",\n",
    "           metrics=[keras.metrics.MeanSquaredError()])\n",
    "\n",
    "history = nn.fit(train_X, train_y, batch_size=64, epochs=700, verbose=0)\n",
    "\n",
    "pred = nn.predict(test_X)\n",
    "\n",
    "print(history.history['mean_squared_error'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# showing graph with first 100 values\n",
    "plt.figure(figsize=(50,50))\n",
    "line1 = plt.plot(range(100), pred[:100],linewidth=1, label='prediction')\n",
    "line2 = plt.plot(range(100), train_y[:100], label='actual')\n",
    "plt.legend(loc=\"upper left\", prop={'size': 60})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source https://www.pyimagesearch.com/2019/01/21/regression-with-keras/\n",
    "diff = pred.flatten() - train_y\n",
    "percent_diff = (diff/train_y)*100\n",
    "abs_percent_diff = np.abs(percent_diff)\n",
    "mean = np.mean(abs_percent_diff)\n",
    "std = np.std(abs_percent_diff)\n",
    "print(\"Mean Abs. Percent Diff: \" + str(mean))\n",
    "print(\"Std. Deviation        : \" + str(std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate original XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize XGBoost model with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Random Forest model with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Bayesian Neural Networks\n",
    "## Train and test a Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code  source for BNN https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from blitz.modules.base_bayesian_module import BayesianModule\n",
    "\n",
    "from blitz.modules.weight_sampler import TrainableRandomDistribution\n",
    "from blitz.losses import kl_divergence_from_nn\n",
    "from blitz.modules.base_bayesian_module import BayesianModule, BayesianRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(\"critical_temp\", axis = 1)\n",
    "y = train['critical_temp']\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(np.expand_dims(y, -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train).float(), torch.tensor(y_train).float()\n",
    "X_test, y_test = torch.tensor(X_test).float(), torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14884, 81])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variational_estimator\n",
    "class BayesianRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        #self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.blinear1 = BayesianLinear(input_dim, 100)\n",
    "        self.blinear2 = BayesianLinear(100, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = self.blinear1(x)\n",
    "        return self.blinear2(x_)\n",
    "    \n",
    "    \n",
    "#from https://github.com/piEsposito/blitz-bayesian-deep-learning/blob/3526b57058e0914105e17cf7e93682ef11a9cd28/blitz/utils/variational_estimator.py#L35    \n",
    "    def nn_kl_divergence(self):\n",
    "        \"\"\"Returns the sum of the KL divergence of each of the BayesianModules of the model, which are from\n",
    "            their posterior current distribution of weights relative to a scale-mixtured prior (and simpler) distribution of weights\n",
    "            Parameters:\n",
    "                N/a\n",
    "            Returns torch.tensor with 0 dim.      \n",
    "        \n",
    "        \"\"\"\n",
    "        return kl_divergence_from_nn(self)\n",
    "    \n",
    "    def sample_elbo(self,\n",
    "                    inputs,\n",
    "                    labels,\n",
    "                    criterion,\n",
    "                    sample_nbr,\n",
    "                    complexity_cost_weight=1):\n",
    "\n",
    "        \"\"\" Samples the ELBO Loss for a batch of data, consisting of inputs and corresponding-by-index labels\n",
    "                The ELBO Loss consists of the sum of the KL Divergence of the model \n",
    "                 (explained above, interpreted as a \"complexity part\" of the loss)\n",
    "                 with the actual criterion - (loss function) of optimization of our model\n",
    "                 (the performance part of the loss). \n",
    "                As we are using variational inference, it takes several (quantified by the parameter sample_nbr) Monte-Carlo\n",
    "                 samples of the weights in order to gather a better approximation for the loss.\n",
    "            Parameters:\n",
    "                inputs: torch.tensor -> the input data to the model\n",
    "                labels: torch.tensor -> label data for the performance-part of the loss calculation\n",
    "                        The shape of the labels must match the label-parameter shape of the criterion (one hot encoded or as index, if needed)\n",
    "                criterion: torch.nn.Module, custom criterion (loss) function, torch.nn.functional function -> criterion to gather\n",
    "                            the performance cost for the model\n",
    "                sample_nbr: int -> The number of times of the weight-sampling and predictions done in our Monte-Carlo approach to \n",
    "                            gather the loss to be .backwarded in the optimization of the model.        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        loss = 0\n",
    "        for _ in range(sample_nbr):\n",
    "            outputs = self(inputs)\n",
    "            loss += criterion(outputs, labels) \n",
    "            loss += self.nn_kl_divergence() * complexity_cost_weight\n",
    "            \n",
    "        return loss / sample_nbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(regressor,\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        samples = 100,\n",
    "                        std_multiplier = 2):\n",
    "    preds = [regressor(X) for i in range(samples)]\n",
    "    preds = torch.stack(preds)\n",
    "    means = preds.mean(axis=0)\n",
    "    stds = preds.std(axis=0)\n",
    "    ci_upper = means + (std_multiplier * stds)\n",
    "    ci_lower = means - (std_multiplier * stds)\n",
    "    ic_acc = (ci_lower <= y) * (ci_upper >= y)\n",
    "    ic_acc = ic_acc.float().mean()\n",
    "    return ic_acc, (ci_upper >= y).float().mean(), (ci_lower <= y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = BayesianRegressor(81, 1)\n",
    "optimizer = optim.Adam(regressor.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader_train = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "dataloader_test = torch.utils.data.DataLoader(ds_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-617b86cec658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             ic_acc, under_ci_upper, over_ci_lower = evaluate_regression(regressor,\n\u001b[0m\u001b[1;32m     17\u001b[0m                                                                         \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                                         \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ba80d0e13430>\u001b[0m in \u001b[0;36mevaluate_regression\u001b[0;34m(regressor, X_train, y_train, samples, std_multiplier)\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         std_multiplier = 2):\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ba80d0e13430>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         std_multiplier = 2):\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-dc334eeca366>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/blitz/modules/linear_bayesian_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_prior_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_log_prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_frozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "\n",
    "iteration = 0\n",
    "for epoch in range(100):\n",
    "    for i, (datapoints, labels) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = regressor.sample_elbo(inputs=datapoints,\n",
    "                           labels=labels,\n",
    "                           criterion=criterion,\n",
    "                           sample_nbr=3)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration%100==0:\n",
    "            ic_acc, under_ci_upper, over_ci_lower = evaluate_regression(regressor,\n",
    "                                                                        X_test,\n",
    "                                                                        y_test,\n",
    "                                                                        samples=32,\n",
    "                                                                        std_multiplier=3)\n",
    "            \n",
    "            print(\"CI acc: {:.2f}, CI upper acc: {:.2f}, CI lower acc: {:.2f}\".format(ic_acc, under_ci_upper, over_ci_lower))\n",
    "            print(\"Loss: {:.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Bayesian Neural Network with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze all model results and performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frontend?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
