{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ECS 171 Group Project - Group 10 - ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean/Wrangle Data\n",
    "\n",
    "\"Process the data the same way the paper author did\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data\n",
      "(21263, 82)\n",
      "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
      "0                   4         88.944468             57.862692   \n",
      "1                   5         92.729214             58.518416   \n",
      "2                   4         88.944468             57.885242   \n",
      "3                   4         88.944468             57.873967   \n",
      "4                   4         88.944468             57.840143   \n",
      "\n",
      "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
      "0          66.361592              36.116612             1.181795   \n",
      "1          73.132787              36.396602             1.449309   \n",
      "2          66.361592              36.122509             1.181795   \n",
      "3          66.361592              36.119560             1.181795   \n",
      "4          66.361592              36.110716             1.181795   \n",
      "\n",
      "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
      "0                 1.062396          122.90607              31.794921   \n",
      "1                 1.057755          122.90607              36.161939   \n",
      "2                 0.975980          122.90607              35.741099   \n",
      "3                 1.022291          122.90607              33.768010   \n",
      "4                 1.129224          122.90607              27.848743   \n",
      "\n",
      "   std_atomic_mass  ...  wtd_mean_Valence  gmean_Valence  wtd_gmean_Valence  \\\n",
      "0        51.968828  ...          2.257143       2.213364           2.219783   \n",
      "1        47.094633  ...          2.257143       1.888175           2.210679   \n",
      "2        51.968828  ...          2.271429       2.213364           2.232679   \n",
      "3        51.968828  ...          2.264286       2.213364           2.226222   \n",
      "4        51.968828  ...          2.242857       2.213364           2.206963   \n",
      "\n",
      "   entropy_Valence  wtd_entropy_Valence  range_Valence  wtd_range_Valence  \\\n",
      "0         1.368922             1.066221              1           1.085714   \n",
      "1         1.557113             1.047221              2           1.128571   \n",
      "2         1.368922             1.029175              1           1.114286   \n",
      "3         1.368922             1.048834              1           1.100000   \n",
      "4         1.368922             1.096052              1           1.057143   \n",
      "\n",
      "   std_Valence  wtd_std_Valence  critical_temp  \n",
      "0     0.433013         0.437059           29.0  \n",
      "1     0.632456         0.468606           26.0  \n",
      "2     0.433013         0.444697           19.0  \n",
      "3     0.433013         0.440952           22.0  \n",
      "4     0.433013         0.428809           23.0  \n",
      "\n",
      "[5 rows x 82 columns]\n",
      "\n",
      "Chemical formulas\n",
      "(21263, 88)\n",
      "     H  He   Li   Be    B    C    N    O    F  Ne  ...   Au   Hg   Tl   Pb  \\\n",
      "0  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n",
      "1  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n",
      "2  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n",
      "3  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n",
      "4  0.0   0  0.0  0.0  0.0  0.0  0.0  4.0  0.0   0  ...  0.0  0.0  0.0  0.0   \n",
      "\n",
      "    Bi  Po  At  Rn  critical_temp                material  \n",
      "0  0.0   0   0   0           29.0         Ba0.2La1.8Cu1O4  \n",
      "1  0.0   0   0   0           26.0  Ba0.1La1.9Ag0.1Cu0.9O4  \n",
      "2  0.0   0   0   0           19.0         Ba0.1La1.9Cu1O4  \n",
      "3  0.0   0   0   0           22.0       Ba0.15La1.85Cu1O4  \n",
      "4  0.0   0   0   0           23.0         Ba0.3La1.7Cu1O4  \n",
      "\n",
      "[5 rows x 88 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Contains 81 features extracted from 21263 superconductors along with the critical temperature in the 82nd column\n",
    "train = pd.read_csv('./../superconduct/train.csv')\n",
    "\n",
    "#Contains the chemical formula broken up for all the 21263 superconductors from the train.csv file.\n",
    "#The last two columns have the critical temperature and chemical formula.\n",
    "formula = pd.read_csv('./../superconduct/unique_m.csv')\n",
    "\n",
    "print(\"Training Data\")\n",
    "print(train.shape)\n",
    "print(train.head())\n",
    "\n",
    "print(\"\\nChemical formulas\")\n",
    "print(formula.shape)\n",
    "print(formula.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicate original XGBoost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize XGBoost model with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test a Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_Valence</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "      <td>21263.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.115224</td>\n",
       "      <td>87.557631</td>\n",
       "      <td>72.988310</td>\n",
       "      <td>71.290627</td>\n",
       "      <td>58.539916</td>\n",
       "      <td>1.165608</td>\n",
       "      <td>1.063884</td>\n",
       "      <td>115.601251</td>\n",
       "      <td>33.225218</td>\n",
       "      <td>44.391893</td>\n",
       "      <td>...</td>\n",
       "      <td>3.198228</td>\n",
       "      <td>3.153127</td>\n",
       "      <td>3.056536</td>\n",
       "      <td>3.055885</td>\n",
       "      <td>1.295682</td>\n",
       "      <td>1.052841</td>\n",
       "      <td>2.041010</td>\n",
       "      <td>1.483007</td>\n",
       "      <td>0.839342</td>\n",
       "      <td>0.673987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.439295</td>\n",
       "      <td>29.676497</td>\n",
       "      <td>33.490406</td>\n",
       "      <td>31.030272</td>\n",
       "      <td>36.651067</td>\n",
       "      <td>0.364930</td>\n",
       "      <td>0.401423</td>\n",
       "      <td>54.626887</td>\n",
       "      <td>26.967752</td>\n",
       "      <td>20.035430</td>\n",
       "      <td>...</td>\n",
       "      <td>1.044611</td>\n",
       "      <td>1.191249</td>\n",
       "      <td>1.046257</td>\n",
       "      <td>1.174815</td>\n",
       "      <td>0.393155</td>\n",
       "      <td>0.380291</td>\n",
       "      <td>1.242345</td>\n",
       "      <td>0.978176</td>\n",
       "      <td>0.484676</td>\n",
       "      <td>0.455580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.941000</td>\n",
       "      <td>6.423452</td>\n",
       "      <td>5.320573</td>\n",
       "      <td>1.960849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>72.458076</td>\n",
       "      <td>52.143839</td>\n",
       "      <td>58.041225</td>\n",
       "      <td>35.248990</td>\n",
       "      <td>0.966676</td>\n",
       "      <td>0.775363</td>\n",
       "      <td>78.512902</td>\n",
       "      <td>16.824174</td>\n",
       "      <td>32.890369</td>\n",
       "      <td>...</td>\n",
       "      <td>2.333333</td>\n",
       "      <td>2.116732</td>\n",
       "      <td>2.279705</td>\n",
       "      <td>2.091251</td>\n",
       "      <td>1.060857</td>\n",
       "      <td>0.775678</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921454</td>\n",
       "      <td>0.451754</td>\n",
       "      <td>0.306892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>84.922750</td>\n",
       "      <td>60.696571</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>39.918385</td>\n",
       "      <td>1.199541</td>\n",
       "      <td>1.146783</td>\n",
       "      <td>122.906070</td>\n",
       "      <td>26.636008</td>\n",
       "      <td>45.123500</td>\n",
       "      <td>...</td>\n",
       "      <td>2.833333</td>\n",
       "      <td>2.618182</td>\n",
       "      <td>2.615321</td>\n",
       "      <td>2.434057</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.166532</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.063077</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>100.404410</td>\n",
       "      <td>86.103540</td>\n",
       "      <td>78.116681</td>\n",
       "      <td>73.113234</td>\n",
       "      <td>1.444537</td>\n",
       "      <td>1.359418</td>\n",
       "      <td>154.119320</td>\n",
       "      <td>38.356908</td>\n",
       "      <td>59.322812</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.026201</td>\n",
       "      <td>3.727919</td>\n",
       "      <td>3.914868</td>\n",
       "      <td>1.589027</td>\n",
       "      <td>1.330801</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.918400</td>\n",
       "      <td>1.200000</td>\n",
       "      <td>1.020436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.000000</td>\n",
       "      <td>208.980400</td>\n",
       "      <td>208.980400</td>\n",
       "      <td>208.980400</td>\n",
       "      <td>208.980400</td>\n",
       "      <td>1.983797</td>\n",
       "      <td>1.958203</td>\n",
       "      <td>207.972460</td>\n",
       "      <td>205.589910</td>\n",
       "      <td>101.019700</td>\n",
       "      <td>...</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>2.141963</td>\n",
       "      <td>1.949739</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.992200</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "count        21263.000000      21263.000000          21263.000000   \n",
       "mean             4.115224         87.557631             72.988310   \n",
       "std              1.439295         29.676497             33.490406   \n",
       "min              1.000000          6.941000              6.423452   \n",
       "25%              3.000000         72.458076             52.143839   \n",
       "50%              4.000000         84.922750             60.696571   \n",
       "75%              5.000000        100.404410             86.103540   \n",
       "max              9.000000        208.980400            208.980400   \n",
       "\n",
       "       gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "count       21263.000000           21263.000000         21263.000000   \n",
       "mean           71.290627              58.539916             1.165608   \n",
       "std            31.030272              36.651067             0.364930   \n",
       "min             5.320573               1.960849             0.000000   \n",
       "25%            58.041225              35.248990             0.966676   \n",
       "50%            66.361592              39.918385             1.199541   \n",
       "75%            78.116681              73.113234             1.444537   \n",
       "max           208.980400             208.980400             1.983797   \n",
       "\n",
       "       wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "count             21263.000000       21263.000000           21263.000000   \n",
       "mean                  1.063884         115.601251              33.225218   \n",
       "std                   0.401423          54.626887              26.967752   \n",
       "min                   0.000000           0.000000               0.000000   \n",
       "25%                   0.775363          78.512902              16.824174   \n",
       "50%                   1.146783         122.906070              26.636008   \n",
       "75%                   1.359418         154.119320              38.356908   \n",
       "max                   1.958203         207.972460             205.589910   \n",
       "\n",
       "       std_atomic_mass  ...  mean_Valence  wtd_mean_Valence  gmean_Valence  \\\n",
       "count     21263.000000  ...  21263.000000      21263.000000   21263.000000   \n",
       "mean         44.391893  ...      3.198228          3.153127       3.056536   \n",
       "std          20.035430  ...      1.044611          1.191249       1.046257   \n",
       "min           0.000000  ...      1.000000          1.000000       1.000000   \n",
       "25%          32.890369  ...      2.333333          2.116732       2.279705   \n",
       "50%          45.123500  ...      2.833333          2.618182       2.615321   \n",
       "75%          59.322812  ...      4.000000          4.026201       3.727919   \n",
       "max         101.019700  ...      7.000000          7.000000       7.000000   \n",
       "\n",
       "       wtd_gmean_Valence  entropy_Valence  wtd_entropy_Valence  range_Valence  \\\n",
       "count       21263.000000     21263.000000         21263.000000   21263.000000   \n",
       "mean            3.055885         1.295682             1.052841       2.041010   \n",
       "std             1.174815         0.393155             0.380291       1.242345   \n",
       "min             1.000000         0.000000             0.000000       0.000000   \n",
       "25%             2.091251         1.060857             0.775678       1.000000   \n",
       "50%             2.434057         1.368922             1.166532       2.000000   \n",
       "75%             3.914868         1.589027             1.330801       3.000000   \n",
       "max             7.000000         2.141963             1.949739       6.000000   \n",
       "\n",
       "       wtd_range_Valence   std_Valence  wtd_std_Valence  \n",
       "count       21263.000000  21263.000000     21263.000000  \n",
       "mean            1.483007      0.839342         0.673987  \n",
       "std             0.978176      0.484676         0.455580  \n",
       "min             0.000000      0.000000         0.000000  \n",
       "25%             0.921454      0.451754         0.306892  \n",
       "50%             1.063077      0.800000         0.500000  \n",
       "75%             1.918400      1.200000         1.020436  \n",
       "max             6.992200      3.000000         3.000000  \n",
       "\n",
       "[8 rows x 81 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sources: https://www.youtube.com/watch?v=0GrciaGYzV0\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "X = train.drop(\"critical_temp\", axis = 1)\n",
    "y = train['critical_temp']\n",
    "\n",
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_of_elements</th>\n",
       "      <th>mean_atomic_mass</th>\n",
       "      <th>wtd_mean_atomic_mass</th>\n",
       "      <th>gmean_atomic_mass</th>\n",
       "      <th>wtd_gmean_atomic_mass</th>\n",
       "      <th>entropy_atomic_mass</th>\n",
       "      <th>wtd_entropy_atomic_mass</th>\n",
       "      <th>range_atomic_mass</th>\n",
       "      <th>wtd_range_atomic_mass</th>\n",
       "      <th>std_atomic_mass</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_Valence</th>\n",
       "      <th>wtd_mean_Valence</th>\n",
       "      <th>gmean_Valence</th>\n",
       "      <th>wtd_gmean_Valence</th>\n",
       "      <th>entropy_Valence</th>\n",
       "      <th>wtd_entropy_Valence</th>\n",
       "      <th>range_Valence</th>\n",
       "      <th>wtd_range_Valence</th>\n",
       "      <th>std_Valence</th>\n",
       "      <th>wtd_std_Valence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.862692</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.116612</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.062396</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>31.794921</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.219783</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.066221</td>\n",
       "      <td>1</td>\n",
       "      <td>1.085714</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.437059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>92.729214</td>\n",
       "      <td>58.518416</td>\n",
       "      <td>73.132787</td>\n",
       "      <td>36.396602</td>\n",
       "      <td>1.449309</td>\n",
       "      <td>1.057755</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>36.161939</td>\n",
       "      <td>47.094633</td>\n",
       "      <td>...</td>\n",
       "      <td>2.00</td>\n",
       "      <td>2.257143</td>\n",
       "      <td>1.888175</td>\n",
       "      <td>2.210679</td>\n",
       "      <td>1.557113</td>\n",
       "      <td>1.047221</td>\n",
       "      <td>2</td>\n",
       "      <td>1.128571</td>\n",
       "      <td>0.632456</td>\n",
       "      <td>0.468606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.885242</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.122509</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>0.975980</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>35.741099</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.271429</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.232679</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.029175</td>\n",
       "      <td>1</td>\n",
       "      <td>1.114286</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.444697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.873967</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.119560</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.022291</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>33.768010</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.264286</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.226222</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.048834</td>\n",
       "      <td>1</td>\n",
       "      <td>1.100000</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.440952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>88.944468</td>\n",
       "      <td>57.840143</td>\n",
       "      <td>66.361592</td>\n",
       "      <td>36.110716</td>\n",
       "      <td>1.181795</td>\n",
       "      <td>1.129224</td>\n",
       "      <td>122.90607</td>\n",
       "      <td>27.848743</td>\n",
       "      <td>51.968828</td>\n",
       "      <td>...</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2.242857</td>\n",
       "      <td>2.213364</td>\n",
       "      <td>2.206963</td>\n",
       "      <td>1.368922</td>\n",
       "      <td>1.096052</td>\n",
       "      <td>1</td>\n",
       "      <td>1.057143</td>\n",
       "      <td>0.433013</td>\n",
       "      <td>0.428809</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_of_elements  mean_atomic_mass  wtd_mean_atomic_mass  \\\n",
       "0                   4         88.944468             57.862692   \n",
       "1                   5         92.729214             58.518416   \n",
       "2                   4         88.944468             57.885242   \n",
       "3                   4         88.944468             57.873967   \n",
       "4                   4         88.944468             57.840143   \n",
       "\n",
       "   gmean_atomic_mass  wtd_gmean_atomic_mass  entropy_atomic_mass  \\\n",
       "0          66.361592              36.116612             1.181795   \n",
       "1          73.132787              36.396602             1.449309   \n",
       "2          66.361592              36.122509             1.181795   \n",
       "3          66.361592              36.119560             1.181795   \n",
       "4          66.361592              36.110716             1.181795   \n",
       "\n",
       "   wtd_entropy_atomic_mass  range_atomic_mass  wtd_range_atomic_mass  \\\n",
       "0                 1.062396          122.90607              31.794921   \n",
       "1                 1.057755          122.90607              36.161939   \n",
       "2                 0.975980          122.90607              35.741099   \n",
       "3                 1.022291          122.90607              33.768010   \n",
       "4                 1.129224          122.90607              27.848743   \n",
       "\n",
       "   std_atomic_mass  ...  mean_Valence  wtd_mean_Valence  gmean_Valence  \\\n",
       "0        51.968828  ...          2.25          2.257143       2.213364   \n",
       "1        47.094633  ...          2.00          2.257143       1.888175   \n",
       "2        51.968828  ...          2.25          2.271429       2.213364   \n",
       "3        51.968828  ...          2.25          2.264286       2.213364   \n",
       "4        51.968828  ...          2.25          2.242857       2.213364   \n",
       "\n",
       "   wtd_gmean_Valence  entropy_Valence  wtd_entropy_Valence  range_Valence  \\\n",
       "0           2.219783         1.368922             1.066221              1   \n",
       "1           2.210679         1.557113             1.047221              2   \n",
       "2           2.232679         1.368922             1.029175              1   \n",
       "3           2.226222         1.368922             1.048834              1   \n",
       "4           2.206963         1.368922             1.096052              1   \n",
       "\n",
       "   wtd_range_Valence  std_Valence  wtd_std_Valence  \n",
       "0           1.085714     0.433013         0.437059  \n",
       "1           1.128571     0.632456         0.468606  \n",
       "2           1.114286     0.433013         0.444697  \n",
       "3           1.100000     0.433013         0.440952  \n",
       "4           1.057143     0.433013         0.428809  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_variables = list(X.dtypes[X.dtypes != \"object\"].index)\n",
    "X[numeric_variables].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data preprocessing such as: droppings some columns, normalizing, enchoding and so on....\n",
    "#...\n",
    "#...\n",
    "#..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(oob_score=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# building and fitting the model\n",
    "model = RandomForestRegressor(n_estimators = 100, oob_score = True)\n",
    "model.fit(X[numeric_variables],y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the efficiency of the model\n",
    "model.oob_score_\n",
    "y_oob = model.oob_prediction_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Random Forest model with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Research Bayesian Neural Networks\n",
    "## Train and test a Bayesian Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code  source for BNN https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from blitz.modules import BayesianLinear\n",
    "from blitz.utils import variational_estimator\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from blitz.modules.base_bayesian_module import BayesianModule\n",
    "\n",
    "from blitz.modules.weight_sampler import TrainableRandomDistribution\n",
    "from blitz.losses import kl_divergence_from_nn\n",
    "from blitz.modules.base_bayesian_module import BayesianModule, BayesianRNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(\"critical_temp\", axis = 1)\n",
    "y = train['critical_temp']\n",
    "X = StandardScaler().fit_transform(X)\n",
    "y = StandardScaler().fit_transform(np.expand_dims(y, -1))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = .3,\n",
    "                                                    random_state = 42)\n",
    "\n",
    "\n",
    "X_train, y_train = torch.tensor(X_train).float(), torch.tensor(y_train).float()\n",
    "X_test, y_test = torch.tensor(X_test).float(), torch.tensor(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14884, 81])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#variational_estimator\n",
    "class BayesianRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        #self.linear = nn.Linear(input_dim, output_dim)\n",
    "        self.blinear1 = BayesianLinear(input_dim, 100)\n",
    "        self.blinear2 = BayesianLinear(100, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_ = self.blinear1(x)\n",
    "        return self.blinear2(x_)\n",
    "    \n",
    "    \n",
    "#from https://github.com/piEsposito/blitz-bayesian-deep-learning/blob/3526b57058e0914105e17cf7e93682ef11a9cd28/blitz/utils/variational_estimator.py#L35    \n",
    "    def nn_kl_divergence(self):\n",
    "        \"\"\"Returns the sum of the KL divergence of each of the BayesianModules of the model, which are from\n",
    "            their posterior current distribution of weights relative to a scale-mixtured prior (and simpler) distribution of weights\n",
    "            Parameters:\n",
    "                N/a\n",
    "            Returns torch.tensor with 0 dim.      \n",
    "        \n",
    "        \"\"\"\n",
    "        return kl_divergence_from_nn(self)\n",
    "    \n",
    "    def sample_elbo(self,\n",
    "                    inputs,\n",
    "                    labels,\n",
    "                    criterion,\n",
    "                    sample_nbr,\n",
    "                    complexity_cost_weight=1):\n",
    "\n",
    "        \"\"\" Samples the ELBO Loss for a batch of data, consisting of inputs and corresponding-by-index labels\n",
    "                The ELBO Loss consists of the sum of the KL Divergence of the model \n",
    "                 (explained above, interpreted as a \"complexity part\" of the loss)\n",
    "                 with the actual criterion - (loss function) of optimization of our model\n",
    "                 (the performance part of the loss). \n",
    "                As we are using variational inference, it takes several (quantified by the parameter sample_nbr) Monte-Carlo\n",
    "                 samples of the weights in order to gather a better approximation for the loss.\n",
    "            Parameters:\n",
    "                inputs: torch.tensor -> the input data to the model\n",
    "                labels: torch.tensor -> label data for the performance-part of the loss calculation\n",
    "                        The shape of the labels must match the label-parameter shape of the criterion (one hot encoded or as index, if needed)\n",
    "                criterion: torch.nn.Module, custom criterion (loss) function, torch.nn.functional function -> criterion to gather\n",
    "                            the performance cost for the model\n",
    "                sample_nbr: int -> The number of times of the weight-sampling and predictions done in our Monte-Carlo approach to \n",
    "                            gather the loss to be .backwarded in the optimization of the model.        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        loss = 0\n",
    "        for _ in range(sample_nbr):\n",
    "            outputs = self(inputs)\n",
    "            loss += criterion(outputs, labels) \n",
    "            loss += self.nn_kl_divergence() * complexity_cost_weight\n",
    "            \n",
    "        return loss / sample_nbr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(regressor,\n",
    "                        X_train,\n",
    "                        y_train,\n",
    "                        samples = 100,\n",
    "                        std_multiplier = 2):\n",
    "    preds = [regressor(X) for i in range(samples)]\n",
    "    preds = torch.stack(preds)\n",
    "    means = preds.mean(axis=0)\n",
    "    stds = preds.std(axis=0)\n",
    "    ci_upper = means + (std_multiplier * stds)\n",
    "    ci_lower = means - (std_multiplier * stds)\n",
    "    ic_acc = (ci_lower <= y) * (ci_upper >= y)\n",
    "    ic_acc = ic_acc.float().mean()\n",
    "    return ic_acc, (ci_upper >= y).float().mean(), (ci_lower <= y).float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor = BayesianRegressor(81, 1)\n",
    "optimizer = optim.Adam(regressor.parameters(), lr=0.001)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "ds_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader_train = torch.utils.data.DataLoader(ds_train, batch_size=32, shuffle=True)\n",
    "\n",
    "ds_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "dataloader_test = torch.utils.data.DataLoader(ds_test, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-617b86cec658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             ic_acc, under_ci_upper, over_ci_lower = evaluate_regression(regressor,\n\u001b[0m\u001b[1;32m     17\u001b[0m                                                                         \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                                                                         \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ba80d0e13430>\u001b[0m in \u001b[0;36mevaluate_regression\u001b[0;34m(regressor, X_train, y_train, samples, std_multiplier)\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         std_multiplier = 2):\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-ba80d0e13430>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      4\u001b[0m                         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                         std_multiplier = 2):\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mregressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-dc334eeca366>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/blitz/modules/linear_bayesian_layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_prior_dist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prior\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb_log_prior\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_frozen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhas_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtens_ops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtens_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "\n",
    "iteration = 0\n",
    "for epoch in range(100):\n",
    "    for i, (datapoints, labels) in enumerate(dataloader_train):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = regressor.sample_elbo(inputs=datapoints,\n",
    "                           labels=labels,\n",
    "                           criterion=criterion,\n",
    "                           sample_nbr=3)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        iteration += 1\n",
    "        if iteration%100==0:\n",
    "            ic_acc, under_ci_upper, over_ci_lower = evaluate_regression(regressor,\n",
    "                                                                        X_test,\n",
    "                                                                        y_test,\n",
    "                                                                        samples=32,\n",
    "                                                                        std_multiplier=3)\n",
    "            \n",
    "            print(\"CI acc: {:.2f}, CI upper acc: {:.2f}, CI lower acc: {:.2f}\".format(ic_acc, under_ci_upper, over_ci_lower))\n",
    "            print(\"Loss: {:.4f}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize Bayesian Neural Network with tuned hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze all model results and performances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frontend?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
