{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "projectNotebook.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfS6acwDbHeX"
      },
      "source": [
        "# ECS 171 Group Project - Group 10 - ???"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRu9X1Gwj2YL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f33b4535-d7bc-43bb-9088-4033c9ced2f0"
      },
      "source": [
        "!pip install blitz-bayesian-pytorch\n",
        "!pip install ray\n",
        "!pip install tensorboardX"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting blitz-bayesian-pytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/74/8dc8e93b70da6e247fc28afbd508ff98ffd1aa835cd863fbd727b9d25b12/blitz_bayesian_pytorch-0.2.7-py3-none-any.whl (47kB)\n",
            "\r\u001b[K     |██████▉                         | 10kB 26.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 20kB 16.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 30kB 14.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 40kB 13.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.6/dist-packages (from blitz-bayesian-pytorch) (1.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from blitz-bayesian-pytorch) (1.18.5)\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from blitz-bayesian-pytorch) (0.8.1+cu101)\n",
            "Collecting pillow>=7.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/19/d4c25111d36163698396f93c363114cf1cddbacb24744f6612f25b6aa3d0/Pillow-8.0.1-cp36-cp36m-manylinux1_x86_64.whl (2.2MB)\n",
            "\u001b[K     |████████████████████████████████| 2.2MB 11.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.6/dist-packages (from blitz-bayesian-pytorch) (0.22.2.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (0.16.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.7.0->blitz-bayesian-pytorch) (0.8)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (0.17.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.22.2->blitz-bayesian-pytorch) (1.4.1)\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: pillow, blitz-bayesian-pytorch\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "Successfully installed blitz-bayesian-pytorch-0.2.7 pillow-8.0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting ray\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/87/44476ad712acc1f7957cbf88d307d4a0283a740487cf85d710d0211d0135/ray-1.0.1.post1-cp36-cp36m-manylinux1_x86_64.whl (23.1MB)\n",
            "\u001b[K     |████████████████████████████████| 23.1MB 138kB/s \n",
            "\u001b[?25hRequirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from ray) (1.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from ray) (3.0.12)\n",
            "Requirement already satisfied: grpcio>=1.28.1 in /usr/local/lib/python3.6/dist-packages (from ray) (1.33.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from ray) (2.6.0)\n",
            "Collecting opencensus\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/68/4f407bc0980158001c802222fab17e946728aef13f42e5d80d39dfc9ca67/opencensus-0.7.11-py2.py3-none-any.whl (127kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 54.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: google in /usr/local/lib/python3.6/dist-packages (from ray) (2.0.3)\n",
            "Collecting aiohttp\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/e6/d4b6235d776c9b33f853e603efede5aac5a34f71ca9d3877adb30492eb4e/aiohttp-3.7.3-cp36-cp36m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ray) (2.23.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.6/dist-packages (from ray) (7.1.2)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from ray) (3.12.4)\n",
            "Collecting aioredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/64/1b1612d0a104f21f80eb4c6e1b6075f2e6aba8e228f46f229cfd3fdac859/aioredis-1.3.1-py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.4MB/s \n",
            "\u001b[?25hCollecting py-spy>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8e/a7/ab45c9ee3c4654edda3efbd6b8e2fa4962226718a7e3e3be6e3926bf3617/py_spy-0.3.3-py2.py3-none-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 58.4MB/s \n",
            "\u001b[?25hCollecting gpustat\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/69/d8c849715171aeabd61af7da080fdc60948b5a396d2422f1f4672e43d008/gpustat-0.6.0.tar.gz (78kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 11.4MB/s \n",
            "\u001b[?25hCollecting colorful\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b0/8e/e386e248266952d24d73ed734c2f5513f34d9557032618c8910e605dfaf6/colorful-0.5.4-py2.py3-none-any.whl (201kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 52.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.6/dist-packages (from ray) (1.18.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from ray) (3.13)\n",
            "Requirement already satisfied: prometheus-client>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from ray) (0.9.0)\n",
            "Collecting redis<3.5.0,>=3.3.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/05/1fc7feedc19c123e7a95cfc9e7892eb6cdd2e5df4e9e8af6384349c1cc3d/redis-3.4.1-py2.py3-none-any.whl (71kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.9MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Collecting aiohttp-cors\n",
            "  Downloading https://files.pythonhosted.org/packages/13/e7/e436a0c0eb5127d8b491a9b83ecd2391c6ff7dcd5548dfaec2080a2340fd/aiohttp_cors-0.7.0-py3-none-any.whl\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from grpcio>=1.28.1->ray) (1.15.0)\n",
            "Requirement already satisfied: google-api-core<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from opencensus->ray) (1.16.0)\n",
            "Collecting opencensus-context==0.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/f1/33/990f1bd9e7ee770fc8d3c154fc24743a96f16a0e49e14e1b7540cc2fdd93/opencensus_context-0.1.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from google->ray) (4.6.3)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (3.7.4.3)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/95/f89c97db4d27b24fce7c6fb6ef05228f347e43bbf67e4737a0660c8753fd/multidict-5.0.2-cp36-cp36m-manylinux2014_x86_64.whl (141kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 54.1MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/08/52b26b44bce7b818b410aee37c5e424c9ea420c557bca97dc2adac29b151/yarl-1.6.3-cp36-cp36m-manylinux2014_x86_64.whl (293kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 58.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (20.3.0)\n",
            "Requirement already satisfied: chardet<4.0,>=2.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp->ray) (3.0.4)\n",
            "Collecting idna-ssl>=1.0; python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/46/03/07c4894aae38b0de52b52586b24bf189bb83e4ddabfe2e2c8f2419eec6f4/idna-ssl-1.1.0.tar.gz\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (2020.11.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ray) (2.10)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->ray) (50.3.2)\n",
            "Collecting hiredis\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/7d/6acf1c8d4f2fb327ff6feec000b4c56a20628fbe966a4c7cd16c0b80343c/hiredis-1.1.0-cp36-cp36m-manylinux2010_x86_64.whl (61kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: nvidia-ml-py3>=7.352.0 in /usr/local/lib/python3.6/dist-packages (from gpustat->ray) (7.352.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from gpustat->ray) (5.4.8)\n",
            "Collecting blessings>=1.6\n",
            "  Downloading https://files.pythonhosted.org/packages/03/74/489f85a78247609c6b4f13733cbf3ba0d864b11aa565617b645d6fdf2a4a/blessings-1.7-py3-none-any.whl\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.52.0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (2018.9)\n",
            "Requirement already satisfied: google-auth<2.0dev,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0,>=1.0.0->opencensus->ray) (1.17.2)\n",
            "Collecting contextvars; python_version >= \"3.6\" and python_version < \"3.7\"\n",
            "  Downloading https://files.pythonhosted.org/packages/83/96/55b82d9f13763be9d672622e1b8106c85acb83edd7cc2fa5bc67cd9877e9/contextvars-2.4.tar.gz\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.2.8)\n",
            "Collecting immutables>=0.9\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/99/e0/ea6fd4697120327d26773b5a84853f897a68e33d3f9376b00a8ff96e4f63/immutables-0.14-cp36-cp36m-manylinux1_x86_64.whl (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2.0dev,>=0.4.0->google-api-core<2.0.0,>=1.0.0->opencensus->ray) (0.4.8)\n",
            "Building wheels for collected packages: gpustat, idna-ssl, contextvars\n",
            "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpustat: filename=gpustat-0.6.0-cp36-none-any.whl size=12622 sha256=c364e105a2ca57cab772174cd6e76dff6a66a4f5153d1b136d2c1d29ec425486\n",
            "  Stored in directory: /root/.cache/pip/wheels/48/b4/d5/fb5b7f1d040f2ff20687e3bad6867d63155dbde5a7c10f4293\n",
            "  Building wheel for idna-ssl (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for idna-ssl: filename=idna_ssl-1.1.0-cp36-none-any.whl size=3161 sha256=27837b2bd7b34544bde2adbe742fb7bb1e5141f58ee7b83f090d4849349176a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/00/b3/32d613e19e08a739751dd6bf998cfed277728f8b2127ad4eb7\n",
            "  Building wheel for contextvars (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for contextvars: filename=contextvars-2.4-cp36-none-any.whl size=7666 sha256=4b6e96c8d2051ee7ca9f06a81b6ed4a5e54371e3f8f8d3790fe000cb254ad3ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/7d/68/1ebae2668bda2228686e3c1cf16f2c2384cea6e9334ad5f6de\n",
            "Successfully built gpustat idna-ssl contextvars\n",
            "Installing collected packages: immutables, contextvars, opencensus-context, opencensus, multidict, async-timeout, yarl, idna-ssl, aiohttp, hiredis, aioredis, py-spy, blessings, gpustat, colorful, redis, colorama, aiohttp-cors, ray\n",
            "Successfully installed aiohttp-3.7.3 aiohttp-cors-0.7.0 aioredis-1.3.1 async-timeout-3.0.1 blessings-1.7 colorama-0.4.4 colorful-0.5.4 contextvars-2.4 gpustat-0.6.0 hiredis-1.1.0 idna-ssl-1.1.0 immutables-0.14 multidict-5.0.2 opencensus-0.7.11 opencensus-context-0.1.2 py-spy-0.3.3 ray-1.0.1.post1 redis-3.4.1 yarl-1.6.3\n",
            "Collecting tensorboardX\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "\u001b[K     |████████████████████████████████| 317kB 8.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.2)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ugrnfmNjFSs",
        "outputId": "9cbe16df-b667-4036-ac19-5999b4e5e383"
      },
      "source": [
        "#dont run this unless you are hosting data anad notebook in google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "import os\n",
        "os.chdir('/content/drive/My Drive/171-g1/notebook') #change this path with folder containing your code"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAdA5RVabHeX"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from blitz.modules import BayesianLinear\n",
        "from blitz.utils import variational_estimator\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from blitz.modules.base_bayesian_module import BayesianModule\n",
        "from blitz.modules.weight_sampler import TrainableRandomDistribution\n",
        "from blitz.losses import kl_divergence_from_nn\n",
        "from blitz.modules.base_bayesian_module import BayesianModule, BayesianRNN\n",
        "from hyperopt import hp\n",
        "from ray import tune\n",
        "from ray.tune.suggest.hyperopt import HyperOptSearch\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "np.random.seed(171)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oP5eFjyYbHea"
      },
      "source": [
        "# Code  source for BNN https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd\n",
        "def load_data(data_dir=os.path.abspath(\"/content/drive/My Drive/171-g1/Data/\")):\n",
        "    X_train = pd.read_csv(os.path.join(data_dir,\"X_train.csv\"), delimiter = ',', header = None)\n",
        "    X_test = pd.read_csv(os.path.join(data_dir,\"X_test.csv\"), delimiter = ',', header = None)\n",
        "    y_train = pd.read_csv(os.path.join(data_dir,\"Y_train.csv\"), header = None)\n",
        "    y_test = pd.read_csv(os.path.join(data_dir,\"Y_test.csv\"), header = None)\n",
        "\n",
        "    X_train, y_train = torch.tensor(np.array(X_train)).float(), torch.tensor(np.array(y_train)).float()\n",
        "    X_test, y_test = torch.tensor(np.array(X_test)).float(), torch.tensor(np.array(y_test)).float()\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOT_JyntbHea"
      },
      "source": [
        "#variational_estimator\n",
        "class BayesianRegressor(nn.Module):\n",
        "    def __init__(self, conf):\n",
        "        super().__init__()\n",
        "        #self.linear = nn.Linear(input_dim, output_dim)\n",
        "        self.blinear1 = BayesianLinear(81 #terrible that this is hardcoded but im tryna seem sumn\n",
        "                                       , conf['input_dim'])\n",
        "        self.blinear2 = BayesianLinear(conf['input_dim'], conf['output_dim'])\n",
        "        self.sample_nbr = conf['sample_nbr'] #TODO: same thing\n",
        "        self.criterion = conf['criterion']\n",
        "        self.complexity_cost_weight = conf['complexity_cost_weight']\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x_ = self.blinear1(x)\n",
        "        return self.blinear2(x_)\n",
        "    \n",
        "    \n",
        "#from https://github.com/piEsposito/blitz-bayesian-deep-learning/blob/3526b57058e0914105e17cf7e93682ef11a9cd28/blitz/utils/variational_estimator.py#L35    \n",
        "    def nn_kl_divergence(self):\n",
        "        \"\"\"Returns the sum of the KL divergence of each of the BayesianModules of the model, which are from\n",
        "            their posterior current distribution of weights relative to a scale-mixtured prior (and simpler) distribution of weights\n",
        "            Parameters:\n",
        "                N/a\n",
        "            Returns torch.tensor with 0 dim.      \n",
        "        \n",
        "        \"\"\"\n",
        "        return kl_divergence_from_nn(self)\n",
        "    \n",
        "    def sample_elbo(self,\n",
        "                    inputs,\n",
        "                    labels,\n",
        "                    #criterion,\n",
        "                    #sample_nbr, #hyperparam\n",
        "                    #complexity_cost_weight=1 #hyperparam\n",
        "                    ):\n",
        "\n",
        "        \"\"\" Samples the ELBO Loss for a batch of data, consisting of inputs and corresponding-by-index labels\n",
        "                The ELBO Loss consists of the sum of the KL Divergence of the model \n",
        "                 (explained above, interpreted as a \"complexity part\" of the loss)\n",
        "                 with the actual criterion - (loss function) of optimization of our model\n",
        "                 (the performance part of the loss). \n",
        "                As we are using variational inference, it takes several (quantified by the parameter sample_nbr) Monte-Carlo\n",
        "                 samples of the weights in order to gather a better approximation for the loss.\n",
        "            Parameters:\n",
        "                inputs: torch.tensor -> the input data to the model\n",
        "                labels: torch.tensor -> label data for the performance-part of the loss calculation\n",
        "                        The shape of the labels must match the label-parameter shape of the criterion (one hot encoded or as index, if needed)\n",
        "                criterion: torch.nn.Module, custom criterion (loss) function, torch.nn.functional function -> criterion to gather\n",
        "                            the performance cost for the model\n",
        "                sample_nbr: int -> The number of times of the weight-sampling and predictions done in our Monte-Carlo approach to \n",
        "                            gather the loss to be .backwarded in the optimization of the model.        \n",
        "        \n",
        "        \"\"\"\n",
        "\n",
        "        loss = 0\n",
        "        for _ in range(self.sample_nbr):\n",
        "            outputs = self(inputs)\n",
        "            loss += self.criterion(outputs, labels) \n",
        "            loss += self.nn_kl_divergence() * self.complexity_cost_weight\n",
        "            \n",
        "        return (loss / (self.sample_nbr))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XX_U_CkvFqbQ"
      },
      "source": [
        "def train_model(conf, checkpoint_dir=\"../checkpoint/\", data_dir=os.path.abspath(\"/content/drive/My Drive/171-g1/Data/\")):\n",
        "    net = BayesianRegressor(conf)\n",
        "\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        if torch.cuda.device_count() > 1:\n",
        "            net = nn.DataParallel(net)\n",
        "    net.to(device)\n",
        "    print(\"current device: \" + device)\n",
        "\n",
        "    optimizer = optim.Adam(net.parameters(), lr=conf['opt_lr'])\n",
        "    criterion = conf['criterion']\n",
        "\n",
        "    if checkpoint_dir:\n",
        "        model_state, optimizer_state = torch.load(\n",
        "            os.path.join(checkpoint_dir, \"checkpoint\"))\n",
        "        net.load_state_dict(model_state)\n",
        "        optimizer.load_state_dict(optimizer_state)\n",
        "\n",
        "    X_train, X_test, y_train, y_test=load_data() \n",
        "\n",
        "    ds_train = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "    test_abs = int(len(ds_train) * 0.8)\n",
        "    train_subset, val_subset = random_split(\n",
        "        ds_train, [test_abs, len(ds_train) - test_abs])\n",
        "    trainloader = torch.utils.data.DataLoader(train_subset,\n",
        "                                                    batch_size=conf[\"batch_size\"],\n",
        "                                                    shuffle=True,\n",
        "                                                    num_workers=0) \n",
        "    ds_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "    valloader = torch.utils.data.DataLoader(val_subset,\n",
        "                                            batch_size=conf[\"batch_size\"],\n",
        "                                            shuffle=True,\n",
        "                                            num_workers=0)\n",
        "\n",
        "    for epoch in range(10):  # loop over the dataset multiple times\n",
        "        running_loss = 0.0\n",
        "        epoch_steps = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = net.sample_elbo(inputs=inputs,\n",
        "                           labels=labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            epoch_steps += 1\n",
        "            if i % 2000 == 1999:  # print every 2000 mini-batches\n",
        "                print(\"[%d, %5d] loss: %.3f\" % (epoch + 1, i + 1,\n",
        "                                                running_loss / epoch_steps))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # Validation loss\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        total = 0\n",
        "        correct = 0\n",
        "        for i, data in enumerate(valloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                outputs = net(inputs)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.cpu().numpy()\n",
        "                val_steps += 1\n",
        "\n",
        "        with tune.checkpoint_dir(epoch) as checkpoint_dir:\n",
        "            path = os.path.join(checkpoint_dir, \"checkpoint\")\n",
        "            torch.save((net.state_dict(), optimizer.state_dict()), path)\n",
        "\n",
        "        tune.report(loss=(val_loss / val_steps), accuracy=correct / total)\n",
        "    print(\"Finished Training\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsq40_kRBWsk"
      },
      "source": [
        "## go through and adapt\n",
        "\n",
        "def test_accuracy(net, device=\"cpu\"):\n",
        "    X_train, X_test, y_train, y_test=load_data() \n",
        "\n",
        "    ds_test = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        ds_test, batch_size=4, shuffle=False, num_workers=2)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    return correct / total"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLPVIzTAdb4D"
      },
      "source": [
        "from ray import tune\n",
        "from ray.tune import CLIReporter\n",
        "import torch.nn.functional as F\n",
        "from functools import partial\n",
        "from hyperopt import pyll\n",
        "from hyperopt.pyll.base import scope\n",
        "from hyperopt.pyll.stochastic import sample\n",
        "import tensorboardX\n",
        "from torch.utils.data import random_split"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ywEg8Klo0pdP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "outputId": "89a53a1b-3305-407f-9343-882880a2d6ae"
      },
      "source": [
        "## go through and adapt https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/c24b93738bc036c1b66d0387555bf69a/hyperparameter_tuning_tutorial.ipynb#scrollTo=Hmn9JEYO9MJs\n",
        "def main(num_samples=10, max_num_epochs=10, gpus_per_trial=1):\n",
        "    data_dir = os.path.abspath(\"/content/drive/My Drive/171-g1/Data/\")\n",
        "    load_data(data_dir)\n",
        "    conf = {\n",
        "    'input_dim' : (2 ** scope.int(hp.quniform('input_dim', 2, 9, 1))),\n",
        "    'output_dim' : 1, #(2 ** scope.int(hp.quniform('output_dim', 2, 9, 1))),\n",
        "    'criterion' : torch.nn.MSELoss(),\n",
        "    'epoch' : 10,\n",
        "    'eval_samples' : 100,\n",
        "    'eval_std_multiplier' : 3,\n",
        "    'opt_lr' : hp.uniform('opt_lr', 1e-10, 0.1),\n",
        "    'sample_nbr' : 32,\n",
        "    'std_multiplier' : 3, #hparam(?)\n",
        "    'batch_size' : hp.choice('batch_size', [2, 4, 8, 16]),\n",
        "    'complexity_cost_weight' : 1\n",
        "    }\n",
        "\n",
        "    hyperopt_search = HyperOptSearch(conf, max_concurrent=2) # probably dont need this tbh\n",
        "\n",
        "    scheduler = ASHAScheduler(\n",
        "        metric=\"loss\",\n",
        "        mode=\"min\",\n",
        "        max_t=max_num_epochs,\n",
        "        grace_period=1,\n",
        "        reduction_factor=2)\n",
        "    reporter = CLIReporter(\n",
        "        #parameter_columns=[\"input_dim\", \"output_dim\", \"opt_lr\", \"batch_size\"],\n",
        "        metric_columns=[\"loss\", \"accuracy\", \"training_iteration\"])\n",
        "    result = tune.run(\n",
        "        partial(train_model, data_dir=data_dir),\n",
        "        resources_per_trial={\"cpu\": 2, \"gpu\": gpus_per_trial},\n",
        "        config=conf,\n",
        "        num_samples=num_samples,\n",
        "        search_alg=hyperopt_search,\n",
        "        scheduler=scheduler,\n",
        "        progress_reporter=reporter)\n",
        "\n",
        "    best_trial = result.get_best_trial(\"loss\", \"min\", \"last\")\n",
        "    print(\"Best trial config: {}\".format(best_trial.config))\n",
        "    print(\"Best trial final validation loss: {}\".format(\n",
        "        best_trial.last_result[\"loss\"]))\n",
        "    print(\"Best trial final validation accuracy: {}\".format(\n",
        "        best_trial.last_result[\"accuracy\"]))\n",
        "\n",
        "    best_trained_model = BayesianRegressor(best_trial.config[\"input_dim\"], best_trial.config[\"output_dim\"])\n",
        "    device = \"cpu\"\n",
        "    if torch.cuda.is_available():\n",
        "        device = \"cuda\"\n",
        "        if gpus_per_trial > 1:\n",
        "            best_trained_model = nn.DataParallel(best_trained_model)\n",
        "    best_trained_model.to(device)\n",
        "\n",
        "    best_checkpoint_dir = best_trial.checkpoint.value\n",
        "    model_state, optimizer_state = torch.load(os.path.join(\n",
        "        best_checkpoint_dir, \"checkpoint\"))\n",
        "    best_trained_model.load_state_dict(model_state)\n",
        "\n",
        "\n",
        "    test_acc = test_accuracy(best_trained_model, device)\n",
        "    print(\"Best trial test set accuracy: {}\".format(test_acc))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the number of GPUs per trial here:\n",
        "    main(num_samples=10, max_num_epochs=10, gpus_per_trial=1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020-12-01 06:16:05,223\tWARNING suggestion.py:70 -- DeprecationWarning: `max_concurrent` is deprecated for this search algorithm. Use tune.suggest.ConcurrencyLimiter() instead. This will raise an error in future versions of Ray.\n",
            "2020-12-01 06:16:05,233\tWARNING experiment.py:274 -- No name detected on trainable. Using DEFAULT.\n",
            "2020-12-01 06:16:05,234\tINFO registry.py:65 -- Detected unknown callable for trainable. Converting to class.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "== Status ==\n",
            "Memory usage on this node: 3.7/12.7 GiB\n",
            "Using AsyncHyperBand: num_stopped=0\n",
            "Bracket: Iter 8.000: None | Iter 4.000: None | Iter 2.000: None | Iter 1.000: None\n",
            "Resources requested: 2/2 CPUs, 1/1 GPUs, 0.0/7.18 GiB heap, 0.0/2.44 GiB objects (0/1.0 accelerator_type:T4)\n",
            "Result logdir: /root/ray_results/DEFAULT_2020-12-01_06-16-05\n",
            "Number of trials: 1/10 (1 RUNNING)\n",
            "+------------------+----------+-------+--------------+--------------------------+-------------+---------+----------------+-----------------------+-------------+-----------+--------------+--------------+------------------+\n",
            "| Trial name       | status   | loc   |   batch_size |   complexity_cost_weight | criterion   |   epoch |   eval_samples |   eval_std_multiplier |   input_dim |    opt_lr |   output_dim |   sample_nbr |   std_multiplier |\n",
            "|------------------+----------+-------+--------------+--------------------------+-------------+---------+----------------+-----------------------+-------------+-----------+--------------+--------------+------------------|\n",
            "| DEFAULT_b199bca4 | RUNNING  |       |            4 |                        1 | MSELoss()   |      10 |            100 |                     3 |         512 | 0.0585561 |            1 |           32 |                3 |\n",
            "+------------------+----------+-------+--------------+--------------------------+-------------+---------+----------------+-----------------------+-------------+-----------+--------------+--------------+------------------+\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-df08dc5dc802>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# You can change the number of GPUs per trial here:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_num_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpus_per_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-df08dc5dc802>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(num_samples, max_num_epochs, gpus_per_trial)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0msearch_alg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperopt_search\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         progress_reporter=reporter)\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loss\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"min\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"last\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/tune.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, loggers, log_to_file, trial_name_creator, trial_dirname_creator, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, queue_trials, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, ray_auto_init, run_errored_only, global_checkpoint_period, with_server, upload_dir, sync_to_cloud, sync_to_driver, sync_on_checkpoint)\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mtune_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_finished\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 411\u001b[0;31m         \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    412\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0m_report_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogress_reporter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m                     trial=next_trial)\n\u001b[1;32m    569\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_running_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_no_available_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/trial_runner.py\u001b[0m in \u001b[0;36m_process_events\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m             \u001b[0;31m# TODO(ujvl): Consider combining get_next_available_trial and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m             \u001b[0;31m#  fetch_result functionality so that we don't timeout on fetch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrial_executor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_next_available_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_restoring\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mwarn_if_slow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"process_trial_restore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/tune/ray_trial_executor.py\u001b[0m in \u001b[0;36mget_next_available_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0;31m# See https://github.com/ray-project/ray/issues/4211 for details.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m         \u001b[0;34m[\u001b[0m\u001b[0mresult_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshuffled_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m         \u001b[0mwait_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait_time\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mNONTRIVIAL_WAIT_TIME_THRESHOLD_S\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/ray/worker.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_refs, num_returns, timeout)\u001b[0m\n\u001b[1;32m   1580\u001b[0m             \u001b[0mnum_returns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1581\u001b[0m             \u001b[0mtimeout_milliseconds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1582\u001b[0;31m             \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_task_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1583\u001b[0m         )\n\u001b[1;32m   1584\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mready_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremaining_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.wait\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2LF7Nlv8ZpE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}